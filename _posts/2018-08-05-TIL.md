---
layout: post
title: TIL 180805
---



```
import tensorflow as tf

hello = tf.constant('hello TF')
print(hello)
```

실행하면 return 함.
```
Tensor("Const_1:0", shape=(), dtype=string)
```

이것의 의미는, hello변수의 자료형은 Tensor이고, 이는 상수(Const)를 담고 있다는 의미

### Tensor는, 랭크Rank와 셰이프Shape라는 개념을 가짐

 - 랭크는 차원을 의미 0은 스칼라 1는 벡터 2는 행렬 3이상이면 n차원 Tensor
 - Shape는 각 차원의 요소 갯수, 텐서의 구조를 설명함.


### 아래와 같이 연산(add)를 보자

```
a = tf.constant(10)
b = tf.constant(32)
c = tf.add(a,b)

print(c)
```

여기서 print의 return 은, 42가 나올것 같지만 그렇지 않다. 이는 tensorflow의 작동구조가 두가지로 나뉘어 있기 때문임
1. 그래프의 생성
2. 그래프의 실행

그래프는 텐서플로의 연산 모음이고 이를 실행하는것을 분리함. 함수형 프로그래밍의 특징이기도 함
그래프의 실행은 Session안에서 이뤄져야 하며, Session 객체와 Run 메서드를 사용함

* 그래프 실행의 예제

```
sess = tf.Session() #session의 생성

print(sess.run(hello)) #hello 실행
print(sess.run([a,b,c])) #a,b,c 실행
sess.close() #session 닫기
```


## 플레이스홀더(Placeholder), 변수(Var)

### Placeholder 

사실 이부분은 이해가 가면서 안가는 부분이 있다 일단 해본것만 정리한다

문서의 목적은 선형회귀를 
```
x_data = [1,2,3]
y_data = [1,2,3]

# set variables W, b with in -1.0 ~ 1.0

W = tf.Variable(tf.random_uniform([1],-1.0,1.0))
b = tf.Variable(tf.random_uniform([1],-1.0,1.0))

# set placeholders for data input

X = tf.placeholder(tf.float32, name="X")
Y = tf.placeholder(tf.float32, name="Y")

hypothesis = W + X + b #가설
cost = tf.reduce_mean(tf.square(hypothesis - Y))

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)
train_op = optimizer.minimize(cost)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    
    for step in range(100):
        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y: y_data})
        print(step, cost_val, sess.run(W), sess.run(b))


```